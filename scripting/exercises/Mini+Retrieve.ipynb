{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurs IR-Grundlagen, Praktischer Teil 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Indexing of documents and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path and file definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nt'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name == 'posix':\n",
    "    baseDir =  r'/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/'\n",
    "    doc_path = r'/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/documents/'\n",
    "    query_path = r'/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/queries/'\n",
    "elif os.name == 'nt':\n",
    "    baseDir = r'C:\\ZHAW\\IR\\PT_5_MiniRetrieve\\\\'\n",
    "    doc_path = r'C:\\ZHAW\\IR\\PT_5_MiniRetrieve\\documents\\\\'\n",
    "    \n",
    "STOPWORDS_PATH = 'stopwords.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, read the entire document as a string\n",
    "def readDoc(dir_path, file):\n",
    "    path = dir_path + file\n",
    "    with open(path, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading document and create a string\n",
    "string = readDoc(doc_path, '1')\n",
    "#string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex to parse the string, and perform a simple tokenize\n",
    "# Later will need a proper tokenize function to remove stopwords\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    # Convert string to lowercase\n",
    "    string = string.lower()\n",
    "    # Tokenize using the split_regex definition\n",
    "    raw_tokens = re.split(split_regex, string)\n",
    "    # Remove empty tokens\n",
    "    tokens = []\n",
    "    for raw_token in raw_tokens:\n",
    "        if len(raw_token) != 0:\n",
    "            tokens.append(raw_token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(simpleTokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/stopwords.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(list, 128)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File with stopwords\n",
    "stopfile = os.path.join(baseDir, STOPWORDS_PATH)\n",
    "print(stopfile)\n",
    "# Create list of stopwords\n",
    "stopwords = []\n",
    "with open(stopfile, 'r') as s:\n",
    "    stopwords_string = s.read()\n",
    "    stopwords = re.split(split_regex, stopwords_string)\n",
    "    \n",
    "type(stopwords), len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    tokens = simpleTokenize(string)\n",
    "    # Loop the entire list and add words that are on not on the stopwords list to a new list\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "      if token in stopwords:\n",
    "        continue\n",
    "      else:\n",
    "        filtered.append(token) \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted and Non-Inverted index\n",
    "for each document 'doc' in the list of documents D:\n",
    "\n",
    "    get tokens by tokenizing 'd'\n",
    "    for each token in tokens:\n",
    "        inverted index dict {token_one:[doc with token_one, frequency of token_one in doc with token_one],\n",
    "                             token_two:[doc with token two, frequency of token_two in doc with token_two],\n",
    "                             ...\n",
    "                             }\n",
    "        non-inverted index dict {doc_one:[token in doc_one, frequency of token in doc_one],\n",
    "                                 doc_one:[another_token in doc one, frequency of another_token in doc_one],\n",
    "                                 doc_two:[token in doc_two, frequency of token in doc_two],\n",
    "                                 ...\n",
    "                             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noninvIndex(dir_path, num_files):\n",
    "    \"\"\" A simple implementation of non-inverted index\n",
    "    i.e. token frequency found in document\n",
    "    Args:\n",
    "        dir_path (string): path were all the documents are stored\n",
    "        num_files (string): number of files stored in dir_path; assuming the name is a number\n",
    "    Returns:\n",
    "        docNoniIdx (dict): tokens frequency for each document \n",
    "    \"\"\"\n",
    "    # Create a list of files in the directory\n",
    "    files = []\n",
    "    total_files = int(num_files)\n",
    "    for i in range(1,total_files):\n",
    "        files.append(str(i))\n",
    "    # Dictionary to store the non-inverted index for all documents\n",
    "    docNoniIdx = {}\n",
    "    # Loop the list files to parse all the existing documents\n",
    "    for file in files:\n",
    "        # Create a string for the file read\n",
    "        path = dir_path + file\n",
    "        with open(path, 'r') as f:\n",
    "            string = f.read()\n",
    "        # tokenize of the string removing stopwords\n",
    "        #tokens = tokenize(string)\n",
    "        tokens = simpleTokenize(string)\n",
    "        # With the list of tokens create a non-inverted Index\n",
    "        noniIdx = {}\n",
    "        for token in tokens:\n",
    "            if token not in noniIdx.keys():\n",
    "                noniIdx[token] = 1\n",
    "            else:\n",
    "                noniIdx[token] += 1\n",
    "        docNoniIdx[file] = noniIdx\n",
    "    return docNoniIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_invIndex = noninvIndex(doc_path,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_invIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'a': 7,\n",
       "  'aerodynamics': 1,\n",
       "  'after': 1,\n",
       "  'agree': 1,\n",
       "  'an': 3,\n",
       "  'and': 1,\n",
       "  'angles': 1,\n",
       "  'as': 1,\n",
       "  'at': 2,\n",
       "  'attack': 1,\n",
       "  'basis': 1,\n",
       "  'boundary': 1,\n",
       "  'by': 1,\n",
       "  'comparative': 1,\n",
       "  'configuration': 1,\n",
       "  'control': 1,\n",
       "  'curves': 1,\n",
       "  'destalling': 3,\n",
       "  'determine': 1,\n",
       "  'different': 3,\n",
       "  'distribution': 1,\n",
       "  'due': 2,\n",
       "  'effect': 1,\n",
       "  'effects': 1,\n",
       "  'empirical': 1,\n",
       "  'evaluation': 2,\n",
       "  'evidence': 1,\n",
       "  'experiment': 1,\n",
       "  'experimental': 2,\n",
       "  'flow': 1,\n",
       "  'for': 2,\n",
       "  'found': 1,\n",
       "  'free': 1,\n",
       "  'in': 4,\n",
       "  'increase': 1,\n",
       "  'increment': 2,\n",
       "  'integrated': 1,\n",
       "  'intended': 1,\n",
       "  'investigation': 1,\n",
       "  'layer': 1,\n",
       "  'lift': 4,\n",
       "  'loading': 1,\n",
       "  'made': 2,\n",
       "  'of': 10,\n",
       "  'or': 1,\n",
       "  'order': 1,\n",
       "  'part': 2,\n",
       "  'potential': 1,\n",
       "  'problem': 1,\n",
       "  'produced': 1,\n",
       "  'propeller': 1,\n",
       "  'ratios': 1,\n",
       "  'remaining': 1,\n",
       "  'results': 1,\n",
       "  'showed': 1,\n",
       "  'slipstream': 5,\n",
       "  'span': 1,\n",
       "  'spanwise': 1,\n",
       "  'specific': 1,\n",
       "  'stream': 1,\n",
       "  'study': 1,\n",
       "  'substantial': 1,\n",
       "  'subtracting': 1,\n",
       "  'supporting': 1,\n",
       "  'that': 1,\n",
       "  'the': 12,\n",
       "  'theoretical': 1,\n",
       "  'theory': 1,\n",
       "  'this': 2,\n",
       "  'to': 5,\n",
       "  'together': 1,\n",
       "  'treatments': 1,\n",
       "  'velocity': 1,\n",
       "  'was': 4,\n",
       "  'well': 1,\n",
       "  'were': 1,\n",
       "  'wing': 3,\n",
       "  'with': 2},\n",
       " '2': {'a': 9,\n",
       "  'again': 1,\n",
       "  'an': 2,\n",
       "  'and': 2,\n",
       "  'approximation': 1,\n",
       "  'arises': 1,\n",
       "  'as': 1,\n",
       "  'be': 3,\n",
       "  'been': 1,\n",
       "  'being': 1,\n",
       "  'between': 1,\n",
       "  'body': 2,\n",
       "  'boundary': 5,\n",
       "  'by': 2,\n",
       "  'can': 2,\n",
       "  'classical': 1,\n",
       "  'consequently': 1,\n",
       "  'consider': 1,\n",
       "  'considered': 1,\n",
       "  'constant': 1,\n",
       "  'curved': 1,\n",
       "  'different': 1,\n",
       "  'dimensional': 2,\n",
       "  'discussed': 1,\n",
       "  'discussion': 1,\n",
       "  'edge': 1,\n",
       "  'effects': 1,\n",
       "  'emitting': 1,\n",
       "  'exists': 1,\n",
       "  'feature': 1,\n",
       "  'ferri': 1,\n",
       "  'flat': 3,\n",
       "  'flow': 6,\n",
       "  'fluid': 2,\n",
       "  'for': 1,\n",
       "  'free': 3,\n",
       "  'from': 2,\n",
       "  'has': 1,\n",
       "  'have': 1,\n",
       "  'here': 1,\n",
       "  'high': 1,\n",
       "  'hypersonic': 2,\n",
       "  'in': 7,\n",
       "  'incompressible': 2,\n",
       "  'instance': 1,\n",
       "  'investigated': 1,\n",
       "  'inviscid': 3,\n",
       "  'irrotational': 1,\n",
       "  'is': 5,\n",
       "  'it': 2,\n",
       "  'layer': 5,\n",
       "  'leading': 1,\n",
       "  'libby': 1,\n",
       "  'must': 1,\n",
       "  'necessary': 1,\n",
       "  'nose': 1,\n",
       "  'novel': 1,\n",
       "  'of': 6,\n",
       "  'only': 1,\n",
       "  'or': 1,\n",
       "  'original': 1,\n",
       "  'outside': 1,\n",
       "  'paper': 1,\n",
       "  'past': 4,\n",
       "  'plate': 3,\n",
       "  'possible': 1,\n",
       "  'prandtl': 2,\n",
       "  'present': 1,\n",
       "  'problem': 4,\n",
       "  'recently': 1,\n",
       "  'region': 1,\n",
       "  'restricted': 1,\n",
       "  'rotational': 2,\n",
       "  's': 2,\n",
       "  'shear': 2,\n",
       "  'shock': 2,\n",
       "  'shown': 1,\n",
       "  'simple': 2,\n",
       "  'situation': 2,\n",
       "  'small': 2,\n",
       "  'somewhat': 1,\n",
       "  'speed': 1,\n",
       "  'steady': 1,\n",
       "  'stream': 3,\n",
       "  'study': 2,\n",
       "  'such': 1,\n",
       "  'that': 2,\n",
       "  'the': 18,\n",
       "  'there': 1,\n",
       "  'this': 1,\n",
       "  'to': 2,\n",
       "  'treated': 1,\n",
       "  'two': 2,\n",
       "  'usually': 1,\n",
       "  'viscosity': 2,\n",
       "  'viscous': 2,\n",
       "  'vorticity': 2,\n",
       "  'wave': 2,\n",
       "  'while': 1},\n",
       " '3': {'a': 1,\n",
       "  'are': 1,\n",
       "  'boundary': 2,\n",
       "  'equations': 1,\n",
       "  'flat': 1,\n",
       "  'flow': 2,\n",
       "  'for': 1,\n",
       "  'gradient': 1,\n",
       "  'in': 1,\n",
       "  'incompressible': 1,\n",
       "  'layer': 2,\n",
       "  'no': 1,\n",
       "  'past': 1,\n",
       "  'plate': 1,\n",
       "  'presented': 1,\n",
       "  'pressure': 1,\n",
       "  'shear': 1,\n",
       "  'simple': 1,\n",
       "  'steady': 1,\n",
       "  'the': 2,\n",
       "  'with': 1},\n",
       " '4': {'a': 4,\n",
       "  'also': 1,\n",
       "  'and': 1,\n",
       "  'approximate': 1,\n",
       "  'are': 1,\n",
       "  'been': 1,\n",
       "  'boundary': 5,\n",
       "  'by': 1,\n",
       "  'comparison': 1,\n",
       "  'considered': 1,\n",
       "  'dimensional': 1,\n",
       "  'distribution': 1,\n",
       "  'effect': 1,\n",
       "  'equations': 1,\n",
       "  'flat': 1,\n",
       "  'flow': 3,\n",
       "  'fluid': 1,\n",
       "  'for': 3,\n",
       "  'friction': 1,\n",
       "  'has': 1,\n",
       "  'in': 3,\n",
       "  'incompressible': 2,\n",
       "  'is': 1,\n",
       "  'karman': 1,\n",
       "  'laminar': 1,\n",
       "  'layer': 5,\n",
       "  'made': 1,\n",
       "  'obtained': 1,\n",
       "  'of': 4,\n",
       "  'plate': 2,\n",
       "  'pohlhausen': 1,\n",
       "  'problem': 1,\n",
       "  'shear': 2,\n",
       "  'show': 1,\n",
       "  'skin': 1,\n",
       "  'solutions': 2,\n",
       "  'steady': 1,\n",
       "  'technique': 1,\n",
       "  'the': 8,\n",
       "  'thickness': 1,\n",
       "  'to': 1,\n",
       "  'two': 1,\n",
       "  'uniform': 1,\n",
       "  'velocity': 1,\n",
       "  'vorticity': 1,\n",
       "  'with': 1},\n",
       " '5': {'a': 4,\n",
       "  'aerodynamic': 1,\n",
       "  'analytic': 1,\n",
       "  'are': 1,\n",
       "  'at': 1,\n",
       "  'composite': 1,\n",
       "  'conduction': 2,\n",
       "  'dimensional': 1,\n",
       "  'double': 1,\n",
       "  'during': 1,\n",
       "  'example': 1,\n",
       "  'exposed': 1,\n",
       "  'for': 3,\n",
       "  'heat': 4,\n",
       "  'heating': 2,\n",
       "  'in': 1,\n",
       "  'input': 1,\n",
       "  'internal': 1,\n",
       "  'into': 1,\n",
       "  'layer': 1,\n",
       "  'linear': 1,\n",
       "  'may': 1,\n",
       "  'occur': 1,\n",
       "  'of': 1,\n",
       "  'one': 2,\n",
       "  'presented': 1,\n",
       "  'rate': 2,\n",
       "  'slab': 1,\n",
       "  'slabs': 1,\n",
       "  'small': 1,\n",
       "  'solutions': 1,\n",
       "  'subjected': 1,\n",
       "  'surface': 1,\n",
       "  'the': 1,\n",
       "  'this': 1,\n",
       "  'time': 1,\n",
       "  'to': 2,\n",
       "  'transient': 2,\n",
       "  'triangular': 1,\n",
       "  'type': 1},\n",
       " '6': {'2': 1,\n",
       "  'a': 4,\n",
       "  'analytic': 1,\n",
       "  'and': 2,\n",
       "  'are': 1,\n",
       "  'at': 3,\n",
       "  'briefly': 1,\n",
       "  'by': 1,\n",
       "  'cases': 1,\n",
       "  'contribution': 1,\n",
       "  'dimensional': 1,\n",
       "  'double': 1,\n",
       "  'duration': 1,\n",
       "  'face': 1,\n",
       "  'flow': 1,\n",
       "  'for': 3,\n",
       "  'forum': 1,\n",
       "  'gave': 1,\n",
       "  'general': 1,\n",
       "  'give': 1,\n",
       "  'given': 1,\n",
       "  'heat': 3,\n",
       "  'here': 1,\n",
       "  'his': 1,\n",
       "  'how': 1,\n",
       "  'i': 1,\n",
       "  'in': 3,\n",
       "  'incomplete': 1,\n",
       "  'indicate': 1,\n",
       "  'input': 2,\n",
       "  'insulated': 1,\n",
       "  'interface': 1,\n",
       "  'is': 1,\n",
       "  'it': 1,\n",
       "  'layer': 1,\n",
       "  'longer': 1,\n",
       "  'method': 1,\n",
       "  'multilayer': 1,\n",
       "  'no': 1,\n",
       "  'obtained': 1,\n",
       "  'of': 2,\n",
       "  'one': 2,\n",
       "  'other': 1,\n",
       "  'out': 1,\n",
       "  'particular': 1,\n",
       "  'point': 1,\n",
       "  'problem': 1,\n",
       "  'propose': 1,\n",
       "  'rate': 1,\n",
       "  'readers': 1,\n",
       "  'recent': 1,\n",
       "  'reference': 1,\n",
       "  'resistance': 1,\n",
       "  'slab': 2,\n",
       "  'solution': 1,\n",
       "  'solutions': 3,\n",
       "  'temperature': 1,\n",
       "  'than': 1,\n",
       "  'that': 1,\n",
       "  'the': 10,\n",
       "  'thermal': 1,\n",
       "  'this': 1,\n",
       "  'three': 1,\n",
       "  'times': 1,\n",
       "  'to': 5,\n",
       "  'transient': 1,\n",
       "  'triangular': 1,\n",
       "  'using': 1,\n",
       "  'wassermann': 2,\n",
       "  'were': 1,\n",
       "  'with': 2},\n",
       " '7': {'1': 2,\n",
       "  '12': 1,\n",
       "  '2': 2,\n",
       "  '3': 2,\n",
       "  '4': 1,\n",
       "  '5': 1,\n",
       "  '67': 1,\n",
       "  '71': 1,\n",
       "  '90': 1,\n",
       "  'a': 1,\n",
       "  'after': 1,\n",
       "  'and': 7,\n",
       "  'appears': 1,\n",
       "  'are': 1,\n",
       "  'as': 1,\n",
       "  'at': 4,\n",
       "  'becoming': 1,\n",
       "  'begins': 1,\n",
       "  'boundary': 4,\n",
       "  'breakdown': 2,\n",
       "  'breaks': 1,\n",
       "  'by': 2,\n",
       "  'california': 1,\n",
       "  'contaminates': 1,\n",
       "  'contamination': 1,\n",
       "  'controlled': 1,\n",
       "  'dimensional': 3,\n",
       "  'double': 1,\n",
       "  'down': 1,\n",
       "  'each': 1,\n",
       "  'edge': 1,\n",
       "  'effect': 3,\n",
       "  'element': 1,\n",
       "  'elements': 3,\n",
       "  'ensuing': 1,\n",
       "  'experiments': 1,\n",
       "  'field': 2,\n",
       "  'flow': 3,\n",
       "  'fourth': 1,\n",
       "  'from': 1,\n",
       "  'has': 1,\n",
       "  'height': 2,\n",
       "  'in': 3,\n",
       "  'inch': 1,\n",
       "  'increasing': 1,\n",
       "  'indicate': 1,\n",
       "  'induced': 1,\n",
       "  'initial': 1,\n",
       "  'institute': 1,\n",
       "  'investigate': 1,\n",
       "  'is': 2,\n",
       "  'jet': 1,\n",
       "  'k': 1,\n",
       "  'kinematic': 1,\n",
       "  'laboratory': 1,\n",
       "  'laminar': 2,\n",
       "  'lateral': 1,\n",
       "  'layer': 4,\n",
       "  'little': 1,\n",
       "  'local': 1,\n",
       "  'mach': 1,\n",
       "  'may': 1,\n",
       "  'more': 1,\n",
       "  'number': 3,\n",
       "  'numbers': 1,\n",
       "  'occurs': 1,\n",
       "  'of': 14,\n",
       "  'on': 3,\n",
       "  'one': 1,\n",
       "  'outer': 1,\n",
       "  'per': 1,\n",
       "  'performed': 1,\n",
       "  'persist': 1,\n",
       "  'position': 4,\n",
       "  'power': 1,\n",
       "  'propulsion': 1,\n",
       "  'rather': 1,\n",
       "  'relative': 1,\n",
       "  'results': 1,\n",
       "  'reynolds': 3,\n",
       "  'roughness': 7,\n",
       "  'row': 1,\n",
       "  'size': 1,\n",
       "  'spacing': 2,\n",
       "  'speeds': 1,\n",
       "  'spheres': 1,\n",
       "  'spiral': 2,\n",
       "  'still': 1,\n",
       "  'strength': 1,\n",
       "  'sublayer': 1,\n",
       "  'such': 1,\n",
       "  'suddenly': 1,\n",
       "  'supersonic': 2,\n",
       "  'surrounding': 1,\n",
       "  'tained': 1,\n",
       "  'technology': 1,\n",
       "  'that': 2,\n",
       "  'the': 24,\n",
       "  'thickness': 1,\n",
       "  'three': 3,\n",
       "  'to': 4,\n",
       "  'trailing': 1,\n",
       "  'transition': 5,\n",
       "  'trip': 3,\n",
       "  'tunnel': 1,\n",
       "  'turbulent': 2,\n",
       "  'u': 1,\n",
       "  'upon': 1,\n",
       "  'v': 1,\n",
       "  'varies': 1,\n",
       "  'varying': 1,\n",
       "  'velocity': 1,\n",
       "  'violent': 1,\n",
       "  'viscosity': 1,\n",
       "  'viz': 1,\n",
       "  'vortices': 2,\n",
       "  'vorticity': 2,\n",
       "  'were': 1,\n",
       "  'when': 1,\n",
       "  'where': 2,\n",
       "  'wind': 1,\n",
       "  'with': 1,\n",
       "  'x': 1},\n",
       " '8': {'a': 5,\n",
       "  'additional': 1,\n",
       "  'after': 1,\n",
       "  'an': 1,\n",
       "  'and': 3,\n",
       "  'appeared': 1,\n",
       "  'as': 1,\n",
       "  'at': 3,\n",
       "  'available': 1,\n",
       "  'based': 1,\n",
       "  'basis': 1,\n",
       "  'be': 2,\n",
       "  'behave': 1,\n",
       "  'between': 1,\n",
       "  'boundary': 2,\n",
       "  'bureau': 1,\n",
       "  'conducted': 1,\n",
       "  'could': 1,\n",
       "  'course': 1,\n",
       "  'data': 2,\n",
       "  'desirable': 1,\n",
       "  'dimensional': 5,\n",
       "  'displacement': 1,\n",
       "  'distance': 1,\n",
       "  'dryden': 1,\n",
       "  'during': 1,\n",
       "  'edge': 1,\n",
       "  'effect': 3,\n",
       "  'element': 3,\n",
       "  'elements': 3,\n",
       "  'extend': 1,\n",
       "  'flat': 1,\n",
       "  'found': 1,\n",
       "  'from': 1,\n",
       "  'functional': 1,\n",
       "  'h': 1,\n",
       "  'height': 1,\n",
       "  'higher': 1,\n",
       "  'his': 2,\n",
       "  'in': 3,\n",
       "  'investigation': 1,\n",
       "  'is': 3,\n",
       "  'it': 1,\n",
       "  'l': 1,\n",
       "  'layer': 2,\n",
       "  'leading': 1,\n",
       "  'measurements': 1,\n",
       "  'national': 1,\n",
       "  'number': 1,\n",
       "  'obtained': 2,\n",
       "  'of': 15,\n",
       "  'on': 6,\n",
       "  'plate': 1,\n",
       "  'position': 1,\n",
       "  'primarily': 1,\n",
       "  'range': 1,\n",
       "  'reasonably': 1,\n",
       "  'relation': 1,\n",
       "  'represented': 1,\n",
       "  'results': 1,\n",
       "  'reynolds': 1,\n",
       "  'roughness': 6,\n",
       "  'row': 1,\n",
       "  'same': 1,\n",
       "  'see': 1,\n",
       "  'some': 1,\n",
       "  'standards': 1,\n",
       "  'study': 1,\n",
       "  'such': 1,\n",
       "  'suggestion': 1,\n",
       "  'terms': 1,\n",
       "  'that': 1,\n",
       "  'the': 17,\n",
       "  'thickness': 1,\n",
       "  'three': 2,\n",
       "  'to': 4,\n",
       "  'transition': 4,\n",
       "  'trip': 1,\n",
       "  'two': 3,\n",
       "  'values': 1,\n",
       "  'way': 1,\n",
       "  'well': 1,\n",
       "  'were': 2,\n",
       "  'where': 1,\n",
       "  'whether': 1,\n",
       "  'wire': 1,\n",
       "  'would': 1},\n",
       " '9': {'0': 2,\n",
       "  '1': 1,\n",
       "  '10': 6,\n",
       "  '2': 3,\n",
       "  '26': 1,\n",
       "  '40': 1,\n",
       "  '46': 1,\n",
       "  '5': 9,\n",
       "  '6': 1,\n",
       "  '8': 2,\n",
       "  '9': 1,\n",
       "  'a': 9,\n",
       "  'agreement': 2,\n",
       "  'air': 3,\n",
       "  'along': 1,\n",
       "  'although': 1,\n",
       "  'amount': 1,\n",
       "  'an': 4,\n",
       "  'and': 8,\n",
       "  'angle': 1,\n",
       "  'approximately': 1,\n",
       "  'as': 4,\n",
       "  'at': 9,\n",
       "  'be': 2,\n",
       "  'being': 2,\n",
       "  'below': 1,\n",
       "  'between': 1,\n",
       "  'boundary': 6,\n",
       "  'by': 4,\n",
       "  'caused': 1,\n",
       "  'coefficient': 1,\n",
       "  'compared': 1,\n",
       "  'complete': 1,\n",
       "  'constant': 1,\n",
       "  'contamination': 1,\n",
       "  'degree': 1,\n",
       "  'detection': 1,\n",
       "  'developed': 1,\n",
       "  'dimensional': 1,\n",
       "  'direct': 1,\n",
       "  'discussion': 1,\n",
       "  'disturbances': 1,\n",
       "  'downstream': 1,\n",
       "  'earlier': 1,\n",
       "  'edge': 1,\n",
       "  'effect': 1,\n",
       "  'effective': 1,\n",
       "  'element': 1,\n",
       "  'emphasis': 1,\n",
       "  'extensively': 1,\n",
       "  'far': 1,\n",
       "  'flat': 3,\n",
       "  'floating': 1,\n",
       "  'flow': 3,\n",
       "  'for': 5,\n",
       "  'found': 3,\n",
       "  'friction': 4,\n",
       "  'fully': 1,\n",
       "  'galcit': 1,\n",
       "  'given': 1,\n",
       "  'good': 2,\n",
       "  'greater': 2,\n",
       "  'hastening': 1,\n",
       "  'head': 1,\n",
       "  'hypersonic': 2,\n",
       "  'in': 7,\n",
       "  'incompressible': 1,\n",
       "  'injected': 1,\n",
       "  'injection': 2,\n",
       "  'insulated': 2,\n",
       "  'into': 1,\n",
       "  'investigated': 1,\n",
       "  'investigation': 2,\n",
       "  'is': 1,\n",
       "  'it': 1,\n",
       "  'lacquer': 1,\n",
       "  'laminar': 3,\n",
       "  'layer': 6,\n",
       "  'leading': 1,\n",
       "  'least': 1,\n",
       "  'local': 1,\n",
       "  'low': 2,\n",
       "  'lower': 1,\n",
       "  'mach': 2,\n",
       "  'made': 2,\n",
       "  'means': 2,\n",
       "  'measurements': 3,\n",
       "  'much': 1,\n",
       "  'nominal': 1,\n",
       "  'not': 1,\n",
       "  'number': 3,\n",
       "  'numbers': 6,\n",
       "  'obtained': 2,\n",
       "  'occurred': 1,\n",
       "  'of': 21,\n",
       "  'on': 4,\n",
       "  'only': 1,\n",
       "  'onset': 1,\n",
       "  'originated': 1,\n",
       "  'over': 2,\n",
       "  'phosphorescent': 1,\n",
       "  'pitot': 1,\n",
       "  'plate': 4,\n",
       "  'possibility': 1,\n",
       "  'qualitative': 1,\n",
       "  'r': 1,\n",
       "  'rake': 1,\n",
       "  'range': 2,\n",
       "  'regardless': 1,\n",
       "  'results': 2,\n",
       "  'reynolds': 7,\n",
       "  'shear': 1,\n",
       "  'sidewall': 1,\n",
       "  'skin': 4,\n",
       "  'speed': 1,\n",
       "  'speeds': 1,\n",
       "  'spread': 1,\n",
       "  'stability': 1,\n",
       "  'studies': 1,\n",
       "  'surface': 1,\n",
       "  'surveys': 1,\n",
       "  'technique': 3,\n",
       "  'than': 2,\n",
       "  'that': 2,\n",
       "  'the': 17,\n",
       "  'these': 1,\n",
       "  'this': 2,\n",
       "  'to': 4,\n",
       "  'total': 1,\n",
       "  'transition': 5,\n",
       "  'transverse': 1,\n",
       "  'tunnel': 2,\n",
       "  'turbulent': 4,\n",
       "  'two': 1,\n",
       "  'uniform': 1,\n",
       "  'used': 2,\n",
       "  'value': 2,\n",
       "  'verified': 1,\n",
       "  'was': 10,\n",
       "  'were': 1,\n",
       "  'wind': 1,\n",
       "  'with': 4,\n",
       "  'x': 5}}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_invIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invIndex(dir_path, num_files):\n",
    "    \"\"\" A simple implementation of inverted index\n",
    "    i.e. token frequency found in document\n",
    "    Args:\n",
    "        dir_path (string): path were all the documents are stored\n",
    "        num_files (string): number of files stored in dir_path; assuming the name is a number\n",
    "    Returns:\n",
    "        dociIdx (dict): frequency of token in documents  dociIdx = {token:{'doc 1':freq, 'doc 2':freq}, \n",
    "                                                                    token_two:{'doc 1:freq}, ...} \n",
    "    \"\"\"\n",
    "        # Create a list of files in the directory\n",
    "    files = []\n",
    "    total_files = int(num_files)\n",
    "    for i in range(1,total_files):\n",
    "        files.append(str(i))\n",
    "    # Dictionary to store the non-inverted index for all documents\n",
    "    dociIdx = {}\n",
    "    # Loop the list files to parse all the existing documents\n",
    "    for file in files:\n",
    "        # Create a string for the file read\n",
    "        path = dir_path + file\n",
    "        with open(path, 'r') as f:\n",
    "            string = f.read()\n",
    "        # tokenize of the string; remove stopwords later\n",
    "        tokens = simpleTokenize(string)\n",
    "        # With the list of tokens create a non-inverted Index\n",
    "        for token in tokens:\n",
    "            if token not in dociIdx.keys():\n",
    "                #print(token, 'not in')\n",
    "                dociIdx[token] = {file:1}\n",
    "                #print(dociIdx) \n",
    "            elif token in dociIdx.keys():\n",
    "                #print(token, 'in')\n",
    "                if file in dociIdx[token].keys():\n",
    "                    #print(file, 'in value')\n",
    "                    dociIdx[token][file] += 1 \n",
    "                    #print(dociIdx) \n",
    "                else:\n",
    "                    #print(file, 'not in value')\n",
    "                    dociIdx[token].update({file:1})\n",
    "                    #print(dociIdx) \n",
    "    return dociIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "docinvIndex = invIndex(doc_path,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'9': 2},\n",
       " '1': {'7': 2, '9': 1},\n",
       " '10': {'9': 6},\n",
       " '12': {'7': 1},\n",
       " '2': {'6': 1, '7': 2, '9': 3},\n",
       " '26': {'9': 1},\n",
       " '3': {'7': 2},\n",
       " '4': {'7': 1},\n",
       " '40': {'9': 1},\n",
       " '46': {'9': 1},\n",
       " '5': {'7': 1, '9': 9},\n",
       " '6': {'9': 1},\n",
       " '67': {'7': 1},\n",
       " '71': {'7': 1},\n",
       " '8': {'9': 2},\n",
       " '9': {'9': 1},\n",
       " '90': {'7': 1},\n",
       " 'a': {'1': 7, '2': 9, '3': 1, '4': 4, '5': 4, '6': 4, '7': 1, '8': 5, '9': 9},\n",
       " 'additional': {'8': 1},\n",
       " 'aerodynamic': {'5': 1},\n",
       " 'aerodynamics': {'1': 1},\n",
       " 'after': {'1': 1, '7': 1, '8': 1},\n",
       " 'again': {'2': 1},\n",
       " 'agree': {'1': 1},\n",
       " 'agreement': {'9': 2},\n",
       " 'air': {'9': 3},\n",
       " 'along': {'9': 1},\n",
       " 'also': {'4': 1},\n",
       " 'although': {'9': 1},\n",
       " 'amount': {'9': 1},\n",
       " 'an': {'1': 3, '2': 2, '8': 1, '9': 4},\n",
       " 'analytic': {'5': 1, '6': 1},\n",
       " 'and': {'1': 1, '2': 2, '4': 1, '6': 2, '7': 7, '8': 3, '9': 8},\n",
       " 'angle': {'9': 1},\n",
       " 'angles': {'1': 1},\n",
       " 'appeared': {'8': 1},\n",
       " 'appears': {'7': 1},\n",
       " 'approximate': {'4': 1},\n",
       " 'approximately': {'9': 1},\n",
       " 'approximation': {'2': 1},\n",
       " 'are': {'3': 1, '4': 1, '5': 1, '6': 1, '7': 1},\n",
       " 'arises': {'2': 1},\n",
       " 'as': {'1': 1, '2': 1, '7': 1, '8': 1, '9': 4},\n",
       " 'at': {'1': 2, '5': 1, '6': 3, '7': 4, '8': 3, '9': 9},\n",
       " 'attack': {'1': 1},\n",
       " 'available': {'8': 1},\n",
       " 'based': {'8': 1},\n",
       " 'basis': {'1': 1, '8': 1},\n",
       " 'be': {'2': 3, '8': 2, '9': 2},\n",
       " 'becoming': {'7': 1},\n",
       " 'been': {'2': 1, '4': 1},\n",
       " 'begins': {'7': 1},\n",
       " 'behave': {'8': 1},\n",
       " 'being': {'2': 1, '9': 2},\n",
       " 'below': {'9': 1},\n",
       " 'between': {'2': 1, '8': 1, '9': 1},\n",
       " 'body': {'2': 2},\n",
       " 'boundary': {'1': 1, '2': 5, '3': 2, '4': 5, '7': 4, '8': 2, '9': 6},\n",
       " 'breakdown': {'7': 2},\n",
       " 'breaks': {'7': 1},\n",
       " 'briefly': {'6': 1},\n",
       " 'bureau': {'8': 1},\n",
       " 'by': {'1': 1, '2': 2, '4': 1, '6': 1, '7': 2, '9': 4},\n",
       " 'california': {'7': 1},\n",
       " 'can': {'2': 2},\n",
       " 'cases': {'6': 1},\n",
       " 'caused': {'9': 1},\n",
       " 'classical': {'2': 1},\n",
       " 'coefficient': {'9': 1},\n",
       " 'comparative': {'1': 1},\n",
       " 'compared': {'9': 1},\n",
       " 'comparison': {'4': 1},\n",
       " 'complete': {'9': 1},\n",
       " 'composite': {'5': 1},\n",
       " 'conducted': {'8': 1},\n",
       " 'conduction': {'5': 2},\n",
       " 'configuration': {'1': 1},\n",
       " 'consequently': {'2': 1},\n",
       " 'consider': {'2': 1},\n",
       " 'considered': {'2': 1, '4': 1},\n",
       " 'constant': {'2': 1, '9': 1},\n",
       " 'contaminates': {'7': 1},\n",
       " 'contamination': {'7': 1, '9': 1},\n",
       " 'contribution': {'6': 1},\n",
       " 'control': {'1': 1},\n",
       " 'controlled': {'7': 1},\n",
       " 'could': {'8': 1},\n",
       " 'course': {'8': 1},\n",
       " 'curved': {'2': 1},\n",
       " 'curves': {'1': 1},\n",
       " 'data': {'8': 2},\n",
       " 'degree': {'9': 1},\n",
       " 'desirable': {'8': 1},\n",
       " 'destalling': {'1': 3},\n",
       " 'detection': {'9': 1},\n",
       " 'determine': {'1': 1},\n",
       " 'developed': {'9': 1},\n",
       " 'different': {'1': 3, '2': 1},\n",
       " 'dimensional': {'2': 2, '4': 1, '5': 1, '6': 1, '7': 3, '8': 5, '9': 1},\n",
       " 'direct': {'9': 1},\n",
       " 'discussed': {'2': 1},\n",
       " 'discussion': {'2': 1, '9': 1},\n",
       " 'displacement': {'8': 1},\n",
       " 'distance': {'8': 1},\n",
       " 'distribution': {'1': 1, '4': 1},\n",
       " 'disturbances': {'9': 1},\n",
       " 'double': {'5': 1, '6': 1, '7': 1},\n",
       " 'down': {'7': 1},\n",
       " 'downstream': {'9': 1},\n",
       " 'dryden': {'8': 1},\n",
       " 'due': {'1': 2},\n",
       " 'duration': {'6': 1},\n",
       " 'during': {'5': 1, '8': 1},\n",
       " 'each': {'7': 1},\n",
       " 'earlier': {'9': 1},\n",
       " 'edge': {'2': 1, '7': 1, '8': 1, '9': 1},\n",
       " 'effect': {'1': 1, '4': 1, '7': 3, '8': 3, '9': 1},\n",
       " 'effective': {'9': 1},\n",
       " 'effects': {'1': 1, '2': 1},\n",
       " 'element': {'7': 1, '8': 3, '9': 1},\n",
       " 'elements': {'7': 3, '8': 3},\n",
       " 'emitting': {'2': 1},\n",
       " 'emphasis': {'9': 1},\n",
       " 'empirical': {'1': 1},\n",
       " 'ensuing': {'7': 1},\n",
       " 'equations': {'3': 1, '4': 1},\n",
       " 'evaluation': {'1': 2},\n",
       " 'evidence': {'1': 1},\n",
       " 'example': {'5': 1},\n",
       " 'exists': {'2': 1},\n",
       " 'experiment': {'1': 1},\n",
       " 'experimental': {'1': 2},\n",
       " 'experiments': {'7': 1},\n",
       " 'exposed': {'5': 1},\n",
       " 'extend': {'8': 1},\n",
       " 'extensively': {'9': 1},\n",
       " 'face': {'6': 1},\n",
       " 'far': {'9': 1},\n",
       " 'feature': {'2': 1},\n",
       " 'ferri': {'2': 1},\n",
       " 'field': {'7': 2},\n",
       " 'flat': {'2': 3, '3': 1, '4': 1, '8': 1, '9': 3},\n",
       " 'floating': {'9': 1},\n",
       " 'flow': {'1': 1, '2': 6, '3': 2, '4': 3, '6': 1, '7': 3, '9': 3},\n",
       " 'fluid': {'2': 2, '4': 1},\n",
       " 'for': {'1': 2, '2': 1, '3': 1, '4': 3, '5': 3, '6': 3, '9': 5},\n",
       " 'forum': {'6': 1},\n",
       " 'found': {'1': 1, '8': 1, '9': 3},\n",
       " 'fourth': {'7': 1},\n",
       " 'free': {'1': 1, '2': 3},\n",
       " 'friction': {'4': 1, '9': 4},\n",
       " 'from': {'2': 2, '7': 1, '8': 1},\n",
       " 'fully': {'9': 1},\n",
       " 'functional': {'8': 1},\n",
       " 'galcit': {'9': 1},\n",
       " 'gave': {'6': 1},\n",
       " 'general': {'6': 1},\n",
       " 'give': {'6': 1},\n",
       " 'given': {'6': 1, '9': 1},\n",
       " 'good': {'9': 2},\n",
       " 'gradient': {'3': 1},\n",
       " 'greater': {'9': 2},\n",
       " 'h': {'8': 1},\n",
       " 'has': {'2': 1, '4': 1, '7': 1},\n",
       " 'hastening': {'9': 1},\n",
       " 'have': {'2': 1},\n",
       " 'head': {'9': 1},\n",
       " 'heat': {'5': 4, '6': 3},\n",
       " 'heating': {'5': 2},\n",
       " 'height': {'7': 2, '8': 1},\n",
       " 'here': {'2': 1, '6': 1},\n",
       " 'high': {'2': 1},\n",
       " 'higher': {'8': 1},\n",
       " 'his': {'6': 1, '8': 2},\n",
       " 'how': {'6': 1},\n",
       " 'hypersonic': {'2': 2, '9': 2},\n",
       " 'i': {'6': 1},\n",
       " 'in': {'1': 4,\n",
       "  '2': 7,\n",
       "  '3': 1,\n",
       "  '4': 3,\n",
       "  '5': 1,\n",
       "  '6': 3,\n",
       "  '7': 3,\n",
       "  '8': 3,\n",
       "  '9': 7},\n",
       " 'inch': {'7': 1},\n",
       " 'incomplete': {'6': 1},\n",
       " 'incompressible': {'2': 2, '3': 1, '4': 2, '9': 1},\n",
       " 'increase': {'1': 1},\n",
       " 'increasing': {'7': 1},\n",
       " 'increment': {'1': 2},\n",
       " 'indicate': {'6': 1, '7': 1},\n",
       " 'induced': {'7': 1},\n",
       " 'initial': {'7': 1},\n",
       " 'injected': {'9': 1},\n",
       " 'injection': {'9': 2},\n",
       " 'input': {'5': 1, '6': 2},\n",
       " 'instance': {'2': 1},\n",
       " 'institute': {'7': 1},\n",
       " 'insulated': {'6': 1, '9': 2},\n",
       " 'integrated': {'1': 1},\n",
       " 'intended': {'1': 1},\n",
       " 'interface': {'6': 1},\n",
       " 'internal': {'5': 1},\n",
       " 'into': {'5': 1, '9': 1},\n",
       " 'investigate': {'7': 1},\n",
       " 'investigated': {'2': 1, '9': 1},\n",
       " 'investigation': {'1': 1, '8': 1, '9': 2},\n",
       " 'inviscid': {'2': 3},\n",
       " 'irrotational': {'2': 1},\n",
       " 'is': {'2': 5, '4': 1, '6': 1, '7': 2, '8': 3, '9': 1},\n",
       " 'it': {'2': 2, '6': 1, '8': 1, '9': 1},\n",
       " 'jet': {'7': 1},\n",
       " 'k': {'7': 1},\n",
       " 'karman': {'4': 1},\n",
       " 'kinematic': {'7': 1},\n",
       " 'l': {'8': 1},\n",
       " 'laboratory': {'7': 1},\n",
       " 'lacquer': {'9': 1},\n",
       " 'laminar': {'4': 1, '7': 2, '9': 3},\n",
       " 'lateral': {'7': 1},\n",
       " 'layer': {'1': 1,\n",
       "  '2': 5,\n",
       "  '3': 2,\n",
       "  '4': 5,\n",
       "  '5': 1,\n",
       "  '6': 1,\n",
       "  '7': 4,\n",
       "  '8': 2,\n",
       "  '9': 6},\n",
       " 'leading': {'2': 1, '8': 1, '9': 1},\n",
       " 'least': {'9': 1},\n",
       " 'libby': {'2': 1},\n",
       " 'lift': {'1': 4},\n",
       " 'linear': {'5': 1},\n",
       " 'little': {'7': 1},\n",
       " 'loading': {'1': 1},\n",
       " 'local': {'7': 1, '9': 1},\n",
       " 'longer': {'6': 1},\n",
       " 'low': {'9': 2},\n",
       " 'lower': {'9': 1},\n",
       " 'mach': {'7': 1, '9': 2},\n",
       " 'made': {'1': 2, '4': 1, '9': 2},\n",
       " 'may': {'5': 1, '7': 1},\n",
       " 'means': {'9': 2},\n",
       " 'measurements': {'8': 1, '9': 3},\n",
       " 'method': {'6': 1},\n",
       " 'more': {'7': 1},\n",
       " 'much': {'9': 1},\n",
       " 'multilayer': {'6': 1},\n",
       " 'must': {'2': 1},\n",
       " 'national': {'8': 1},\n",
       " 'necessary': {'2': 1},\n",
       " 'no': {'3': 1, '6': 1},\n",
       " 'nominal': {'9': 1},\n",
       " 'nose': {'2': 1},\n",
       " 'not': {'9': 1},\n",
       " 'novel': {'2': 1},\n",
       " 'number': {'7': 3, '8': 1, '9': 3},\n",
       " 'numbers': {'7': 1, '9': 6},\n",
       " 'obtained': {'4': 1, '6': 1, '8': 2, '9': 2},\n",
       " 'occur': {'5': 1},\n",
       " 'occurred': {'9': 1},\n",
       " 'occurs': {'7': 1},\n",
       " 'of': {'1': 10, '2': 6, '4': 4, '5': 1, '6': 2, '7': 14, '8': 15, '9': 21},\n",
       " 'on': {'7': 3, '8': 6, '9': 4},\n",
       " 'one': {'5': 2, '6': 2, '7': 1},\n",
       " 'only': {'2': 1, '9': 1},\n",
       " 'onset': {'9': 1},\n",
       " 'or': {'1': 1, '2': 1},\n",
       " 'order': {'1': 1},\n",
       " 'original': {'2': 1},\n",
       " 'originated': {'9': 1},\n",
       " 'other': {'6': 1},\n",
       " 'out': {'6': 1},\n",
       " 'outer': {'7': 1},\n",
       " 'outside': {'2': 1},\n",
       " 'over': {'9': 2},\n",
       " 'paper': {'2': 1},\n",
       " 'part': {'1': 2},\n",
       " 'particular': {'6': 1},\n",
       " 'past': {'2': 4, '3': 1},\n",
       " 'per': {'7': 1},\n",
       " 'performed': {'7': 1},\n",
       " 'persist': {'7': 1},\n",
       " 'phosphorescent': {'9': 1},\n",
       " 'pitot': {'9': 1},\n",
       " 'plate': {'2': 3, '3': 1, '4': 2, '8': 1, '9': 4},\n",
       " 'pohlhausen': {'4': 1},\n",
       " 'point': {'6': 1},\n",
       " 'position': {'7': 4, '8': 1},\n",
       " 'possibility': {'9': 1},\n",
       " 'possible': {'2': 1},\n",
       " 'potential': {'1': 1},\n",
       " 'power': {'7': 1},\n",
       " 'prandtl': {'2': 2},\n",
       " 'present': {'2': 1},\n",
       " 'presented': {'3': 1, '5': 1},\n",
       " 'pressure': {'3': 1},\n",
       " 'primarily': {'8': 1},\n",
       " 'problem': {'1': 1, '2': 4, '4': 1, '6': 1},\n",
       " 'produced': {'1': 1},\n",
       " 'propeller': {'1': 1},\n",
       " 'propose': {'6': 1},\n",
       " 'propulsion': {'7': 1},\n",
       " 'qualitative': {'9': 1},\n",
       " 'r': {'9': 1},\n",
       " 'rake': {'9': 1},\n",
       " 'range': {'8': 1, '9': 2},\n",
       " 'rate': {'5': 2, '6': 1},\n",
       " 'rather': {'7': 1},\n",
       " 'ratios': {'1': 1},\n",
       " 'readers': {'6': 1},\n",
       " 'reasonably': {'8': 1},\n",
       " 'recent': {'6': 1},\n",
       " 'recently': {'2': 1},\n",
       " 'reference': {'6': 1},\n",
       " 'regardless': {'9': 1},\n",
       " 'region': {'2': 1},\n",
       " 'relation': {'8': 1},\n",
       " 'relative': {'7': 1},\n",
       " 'remaining': {'1': 1},\n",
       " 'represented': {'8': 1},\n",
       " 'resistance': {'6': 1},\n",
       " 'restricted': {'2': 1},\n",
       " 'results': {'1': 1, '7': 1, '8': 1, '9': 2},\n",
       " 'reynolds': {'7': 3, '8': 1, '9': 7},\n",
       " 'rotational': {'2': 2},\n",
       " 'roughness': {'7': 7, '8': 6},\n",
       " 'row': {'7': 1, '8': 1},\n",
       " 's': {'2': 2},\n",
       " 'same': {'8': 1},\n",
       " 'see': {'8': 1},\n",
       " 'shear': {'2': 2, '3': 1, '4': 2, '9': 1},\n",
       " 'shock': {'2': 2},\n",
       " 'show': {'4': 1},\n",
       " 'showed': {'1': 1},\n",
       " 'shown': {'2': 1},\n",
       " 'sidewall': {'9': 1},\n",
       " 'simple': {'2': 2, '3': 1},\n",
       " 'situation': {'2': 2},\n",
       " 'size': {'7': 1},\n",
       " 'skin': {'4': 1, '9': 4},\n",
       " 'slab': {'5': 1, '6': 2},\n",
       " 'slabs': {'5': 1},\n",
       " 'slipstream': {'1': 5},\n",
       " 'small': {'2': 2, '5': 1},\n",
       " 'solution': {'6': 1},\n",
       " 'solutions': {'4': 2, '5': 1, '6': 3},\n",
       " 'some': {'8': 1},\n",
       " 'somewhat': {'2': 1},\n",
       " 'spacing': {'7': 2},\n",
       " 'span': {'1': 1},\n",
       " 'spanwise': {'1': 1},\n",
       " 'specific': {'1': 1},\n",
       " 'speed': {'2': 1, '9': 1},\n",
       " 'speeds': {'7': 1, '9': 1},\n",
       " 'spheres': {'7': 1},\n",
       " 'spiral': {'7': 2},\n",
       " 'spread': {'9': 1},\n",
       " 'stability': {'9': 1},\n",
       " 'standards': {'8': 1},\n",
       " 'steady': {'2': 1, '3': 1, '4': 1},\n",
       " 'still': {'7': 1},\n",
       " 'stream': {'1': 1, '2': 3},\n",
       " 'strength': {'7': 1},\n",
       " 'studies': {'9': 1},\n",
       " 'study': {'1': 1, '2': 2, '8': 1},\n",
       " 'subjected': {'5': 1},\n",
       " 'sublayer': {'7': 1},\n",
       " 'substantial': {'1': 1},\n",
       " 'subtracting': {'1': 1},\n",
       " 'such': {'2': 1, '7': 1, '8': 1},\n",
       " 'suddenly': {'7': 1},\n",
       " 'suggestion': {'8': 1},\n",
       " 'supersonic': {'7': 2},\n",
       " 'supporting': {'1': 1},\n",
       " 'surface': {'5': 1, '9': 1},\n",
       " 'surrounding': {'7': 1},\n",
       " 'surveys': {'9': 1},\n",
       " 'tained': {'7': 1},\n",
       " 'technique': {'4': 1, '9': 3},\n",
       " 'technology': {'7': 1},\n",
       " 'temperature': {'6': 1},\n",
       " 'terms': {'8': 1},\n",
       " 'than': {'6': 1, '9': 2},\n",
       " 'that': {'1': 1, '2': 2, '6': 1, '7': 2, '8': 1, '9': 2},\n",
       " 'the': {'1': 12,\n",
       "  '2': 18,\n",
       "  '3': 2,\n",
       "  '4': 8,\n",
       "  '5': 1,\n",
       "  '6': 10,\n",
       "  '7': 24,\n",
       "  '8': 17,\n",
       "  '9': 17},\n",
       " 'theoretical': {'1': 1},\n",
       " 'theory': {'1': 1},\n",
       " 'there': {'2': 1},\n",
       " 'thermal': {'6': 1},\n",
       " 'these': {'9': 1},\n",
       " 'thickness': {'4': 1, '7': 1, '8': 1},\n",
       " 'this': {'1': 2, '2': 1, '5': 1, '6': 1, '9': 2},\n",
       " 'three': {'6': 1, '7': 3, '8': 2},\n",
       " 'time': {'5': 1},\n",
       " 'times': {'6': 1},\n",
       " 'to': {'1': 5, '2': 2, '4': 1, '5': 2, '6': 5, '7': 4, '8': 4, '9': 4},\n",
       " 'together': {'1': 1},\n",
       " 'total': {'9': 1},\n",
       " 'trailing': {'7': 1},\n",
       " 'transient': {'5': 2, '6': 1},\n",
       " 'transition': {'7': 5, '8': 4, '9': 5},\n",
       " 'transverse': {'9': 1},\n",
       " 'treated': {'2': 1},\n",
       " 'treatments': {'1': 1},\n",
       " 'triangular': {'5': 1, '6': 1},\n",
       " 'trip': {'7': 3, '8': 1},\n",
       " 'tunnel': {'7': 1, '9': 2},\n",
       " 'turbulent': {'7': 2, '9': 4},\n",
       " 'two': {'2': 2, '4': 1, '8': 3, '9': 1},\n",
       " 'type': {'5': 1},\n",
       " 'u': {'7': 1},\n",
       " 'uniform': {'4': 1, '9': 1},\n",
       " 'upon': {'7': 1},\n",
       " 'used': {'9': 2},\n",
       " 'using': {'6': 1},\n",
       " 'usually': {'2': 1},\n",
       " 'v': {'7': 1},\n",
       " 'value': {'9': 2},\n",
       " 'values': {'8': 1},\n",
       " 'varies': {'7': 1},\n",
       " 'varying': {'7': 1},\n",
       " 'velocity': {'1': 1, '4': 1, '7': 1},\n",
       " 'verified': {'9': 1},\n",
       " 'violent': {'7': 1},\n",
       " 'viscosity': {'2': 2, '7': 1},\n",
       " 'viscous': {'2': 2},\n",
       " 'viz': {'7': 1},\n",
       " 'vortices': {'7': 2},\n",
       " 'vorticity': {'2': 2, '4': 1, '7': 2},\n",
       " 'was': {'1': 4, '9': 10},\n",
       " 'wassermann': {'6': 2},\n",
       " 'wave': {'2': 2},\n",
       " 'way': {'8': 1},\n",
       " 'well': {'1': 1, '8': 1},\n",
       " 'were': {'1': 1, '6': 1, '7': 1, '8': 2, '9': 1},\n",
       " 'when': {'7': 1},\n",
       " 'where': {'7': 2, '8': 1},\n",
       " 'whether': {'8': 1},\n",
       " 'while': {'2': 1},\n",
       " 'wind': {'7': 1, '9': 1},\n",
       " 'wing': {'1': 3},\n",
       " 'wire': {'8': 1},\n",
       " 'with': {'1': 2, '3': 1, '4': 1, '6': 2, '7': 1, '9': 4},\n",
       " 'would': {'8': 1},\n",
       " 'x': {'7': 1, '9': 5}}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docinvIndex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
