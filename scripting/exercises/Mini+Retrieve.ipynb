{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurs IR-Grundlagen, Praktischer Teil 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Indexing of documents and queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path and file definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseDir =  r'/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/'\n",
    "doc_path = r'/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/documents/'\n",
    "query_path = r'/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/queries/'\n",
    "STOPWORDS_PATH = 'stopwords.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, read the entire document as a string\n",
    "def readDoc(dir_path, file):\n",
    "    path = dir_path + file\n",
    "    with open(path, 'r') as f:\n",
    "        string = f.read()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading document and create a string\n",
    "string = readDoc(doc_path, '1')\n",
    "#string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define regex to parse the string, and perform a simple tokenize\n",
    "# Later will need a proper tokenize function to remove stopwords\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    # Convert string to lowercase\n",
    "    string = string.lower()\n",
    "    # Tokenize using the split_regex definition\n",
    "    raw_tokens = re.split(split_regex, string)\n",
    "    # Remove empty tokens\n",
    "    tokens = []\n",
    "    for raw_token in raw_tokens:\n",
    "        if len(raw_token) != 0:\n",
    "            tokens.append(raw_token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(simpleTokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hase/Documents/ZHAW/InfoEng/Lectures/Information_Retrieval/Exercises/PT_5_MiniRetrieve/stopwords.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(list, 128)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File with stopwords\n",
    "stopfile = os.path.join(baseDir, STOPWORDS_PATH)\n",
    "print(stopfile)\n",
    "# Create list of stopwords\n",
    "stopwords = []\n",
    "with open(stopfile, 'r') as s:\n",
    "    stopwords_string = s.read()\n",
    "    stopwords = re.split(split_regex, stopwords_string)\n",
    "    \n",
    "type(stopwords), len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    tokens = simpleTokenize(string)\n",
    "    # Loop the entire list and add words that are on not on the stopwords list to a new list\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "      if token in stopwords:\n",
    "        continue\n",
    "      else:\n",
    "        filtered.append(token) \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted and Non-Inverted index\n",
    "for each document 'doc' in the list of documents D:\n",
    "\n",
    "    get tokens by tokenizing 'd'\n",
    "    for each token in tokens:\n",
    "        inverted index dict {token_one:[doc with token_one, frequency of token_one in doc with token_one],\n",
    "                             token_two:[doc with token two, frequency of token_two in doc with token_two],\n",
    "                             ...\n",
    "                             }\n",
    "        non-inverted index dict {doc_one:[token in doc_one, frequency of token in doc_one],\n",
    "                                 doc_one:[another_token in doc one, frequency of another_token in doc_one],\n",
    "                                 doc_two:[token in doc_two, frequency of token in doc_two],\n",
    "                                 ...\n",
    "                             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noninvIndex(dir_path, num_files):\n",
    "    \"\"\" A simple implementation of inverted index\n",
    "    i.e. token frequency found in document\n",
    "    Args:\n",
    "        dir_path (string): path were all the documents are stored\n",
    "        num_files (string): number of files stored in dir_path; assuming the name is a number\n",
    "    Returns:\n",
    "        list: a list of tuples, or key,value pairs with token,frequency\n",
    "    \"\"\"\n",
    "    # Create a list of files in the directory\n",
    "    files = []\n",
    "    total_files = int(num_files)\n",
    "    for i in range(1,total_files):\n",
    "        files.append(str(i))\n",
    "    # Dictionary to store the non-inverted index for all documents\n",
    "    docNoniIdx = {}\n",
    "    # Loop the list files to parse all the existing documents\n",
    "    for file in files:\n",
    "        # Create a string for the file read\n",
    "        path = dir_path + file\n",
    "        with open(path, 'r') as f:\n",
    "            string = f.read()\n",
    "        # tokenize of the string removing stopwords\n",
    "        tokens = tokenize(string)\n",
    "        # With the list of tokens create a non-inverted Index\n",
    "        noniIdx = {}\n",
    "        for token in tokens:\n",
    "            if token not in noniIdx.keys():\n",
    "                noniIdx[token] = 1\n",
    "            else:\n",
    "                noniIdx[token] += 1\n",
    "        docNoniIdx[file] = noniIdx\n",
    "    return docNoniIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_invIndex = noninvIndex(doc_path,1401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_invIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boundary': 2,\n",
       " 'equations': 1,\n",
       " 'flat': 1,\n",
       " 'flow': 2,\n",
       " 'gradient': 1,\n",
       " 'incompressible': 1,\n",
       " 'layer': 2,\n",
       " 'past': 1,\n",
       " 'plate': 1,\n",
       " 'presented': 1,\n",
       " 'pressure': 1,\n",
       " 'shear': 1,\n",
       " 'simple': 1,\n",
       " 'steady': 1}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_invIndex['3']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
